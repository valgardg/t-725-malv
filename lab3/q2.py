import nltk
import random
from nltk.corpus import udhr
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.feature_extraction.text import CountVectorizer

# Don't change anything in this code cell
from collections import defaultdict
from nltk.corpus import brown

nltk.download('brown')

def get_brown_tags(tag):
  return sorted({w for s in brown_train for w, t in s if t == tag})

def train_model():
  # Create the training set
  word_list = [word for tag_words in words for word in tag_words]
  X = [extract_features(word) for word in word_list]
  y = [tag for tag, tag_words in zip(tags, words) for word in tag_words]

  # Train and evaluate the classifier
  log_clf = LogisticRegression(solver='liblinear', multi_class='ovr')
  log_clf.fit(X, y)
  print("Accuracy: {:.1%}".format(log_clf.score(X, y)))

  # Print the accuracy for each tag
  predictions = log_clf.predict(X)
  errors = defaultdict(list)
  for word, example, label, prediction in zip(word_list, X, y, predictions):
    if label != prediction:
      errors[label].append(word)

  print("\nAccuracy and first 10 errors per tag:")
  for tag, tag_words in zip(tags, words):
    error_words = errors[tag]
    num_total = len(tag_words)
    num_correct = num_total - len(error_words)
    ratio = num_correct / num_total
    print("{:>3} {:,}/{:,} ({:.1%}) {}".format(tag, num_correct, num_total, ratio,
                                              ", ".join(error_words[:10])))

# Download and prepare the Brown corpus for training and testing
brown_train, brown_test = train_test_split(brown.tagged_sents(),
                                           test_size=0.1,
                                           random_state=42)

print("Training sentences: {:,}".format(len(brown_train)))
print("Test sentences: {:,}".format(len(brown_test)))

# Get 1,000 examples of each tag
tags = ['NP', 'NP$', 'VBG', 'VBD']
random.seed(42)
words = [random.sample(get_brown_tags(tag), 1000) for tag in tags]

# Modify the features generated by this function and run the code cell to see
# how your changes affect the accuracy of the classifier.
def extract_features(word):
  special_characters = "!@#$%^&*()-+?_=,<>/'" + '"'
  features = [
      len(word),
      word.endswith('r'),
      word.endswith('ing'), # accuracy went from 37.1% -> 60.1%
      word.endswith('ed'), # accuract went from 60.1% -> 81.3%
      word.endswith("in'"), # ('in') accuracy went from 81.3% -> 81.4%, change to ("in'") accuracy goes from 95.3% -> 96.1%
      word.endswith('rd'),
      'ou' in word, # no change
      word.endswith("'s"), # accuracy went from 81.5% -> 95.3%
      word.endswith("'"), # accuracy went from 96.1% -> 97.6%
      word.endswith(","), # no change
      word[0].isupper(),  # accuracy went from 97.6% -> 99.3%
      word[0].isupper() and word.endswith(","), # no change
      word.isalpha(), # no change
      any(c in special_characters for c in word), # no change
      any(c.isnumeric() for c in word), # accuracy went from 99.3% -> 99.4%
      
  ]

  return features

# The errors listed by this function are words belonging to that tag that were
# incorrectly assigned with another tag. Use them to figure out useful features.
train_model()